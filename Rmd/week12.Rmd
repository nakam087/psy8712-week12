---
title: "week12"
author: "Sarah Nakamoto"
date: "`r Sys.Date()`"
output: pdf_document
---

# Script Settings and Resources
```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#downloaded libraries
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(textstem)
library(qdap)
library(RWeka)
library(wordcloud)
library(ldatuning)
library(topicmodels)
library(tidytext)
library(haven)
library(caret)
```

# Data Import and Cleaning
```{r}
#making the data csv
io_df<-find_thread_urls(subreddit="IOPsychology", period='year') #getting posts from r/IOPsychology for 1 year

content_df<-get_thread_content(io_df$url)#parsing urls from rstats_df to retrieve metadata and comments

#create tibble of titles and number of upvotes (stored within threads of content_df)
week12_tbl <- tibble(
  post = content_df$threads$title,
  upvotes = content_df$threads$upvotes,
)

#writing a csv
#write.csv(week12_tbl,"week12.csv")
```
```{r}
#making a corpus
data<-read_csv("../data/week12.csv")
io_corpus_original<-VCorpus(VectorSource(data$post))

#preprocessing, kept similar order to the example in the slides
# got rid of capitalization, numbers, punctuations, whitespace
#replaced abbreviations
#removed stopwords and io-related things (kept testing with compare_them function until I felt I had most of them)
#didn't include stem processor because it might mess up my word cloud
io_corpus<-io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation))%>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c(stopwords("en"), "io", "industrial organizational psychology", "io psychology", "riopsychology", "io psych", "io-related", "/r/IOpsychology", "industrial and organizational psychology","iop", "iorelated", "organizational psychology", "org psych")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings)) #used this because when tried to specify, ran into errors 

compare_them<-function(corpus_og,corpus_new){
  casenum <- sample(1:length(corpus_og),1)
  print(corpus_og[[casenum]]$content)
  print(corpus_new[[casenum]]$content)
}

compare_them(io_corpus_original,io_corpus) #testing various options 
```
# Analysis
```{r}
# creating DTM
#stole from slides, made bigram
bigram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2))
#ran into errors, must delete empty rows
io_dtm_first <- DocumentTermMatrix(io_corpus, control = list(tokenize = bigram_tokenizer))
tokenCounts <- apply(io_dtm_first, 1, sum)
io_dtm <- io_dtm_first[tokenCounts > 0, ]

#uncomment to view dtm
# io_dtm %>% as.matrix %>% as_tibble %>% View 

#spare terms eliminated ver
io_slim_dtm<- removeSparseTerms(io_dtm,0.998) #started with high specificity, this was the "closest" to the 2:1-3:1 n:k ratio
#io_slim_dtm %>% as.matrix %>% as_tibble %>% View 
#io_slim_dtm$ncol
#io_dtm$ncol
```
```{r}
#tuning an LDA model

#determining the number of topics we need to extract
DTM_tune <- FindTopicsNumber(
  io_dtm,
  topics = seq(2,15,1), #used this from the slides
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
)
FindTopicsNumber_plot(DTM_tune) # it looks like about 5?

#running and exploring the lda
lda_results <-LDA(io_dtm,5) #5 topics
lda_betas<- tidy(lda_results, matrix="beta") #probability a word belongs to topic
lda_gammas<- tidy(lda_results, matrix="gamma") #probability a document contains topic

#making posts into topics
topics_tbl<- tibble(tidy(lda_results,matrix="gamma")%>%
                      group_by(document) %>% #grouping by document
                      top_n(1, gamma) %>% #highest prob per document
                      ungroup() %>% #clear grouping
                      rename(doc_id = document, probability = gamma) %>% #creating doc id
                      mutate(doc_id = as.numeric(doc_id)) %>% 
                      arrange(doc_id)%>% #arrange by doc id
                      mutate(original= week12_tbl$post[doc_id])) #get original posts filtered by doc_id

#answering questions
#lda_betas %>% 
  #group_by(topic)%>%
  #top_n(10,beta)%>%
  #arrange(topic,-beta)%>%
  #View

# Using the beta matrix alone, I would guess that topic 1 are fact/discussion related (discussion, think, etc), topic 2 is related to academia (school, grad), topic 3 is related to applications/events (research, consult, siop), topic 4 is advice (job, career, etc), and topic 5 are opinions (path, idea, etc).

# I feel like some of them are close, but I am interested to see what our analysis brings because I think it's a bit hard to tell how well I did. You might be able to count this as content validity or face validity (which is kind of fake)

```

```{r}
#creating new tibble for analysis
final_tbl<-tibble(topics_tbl,
                  upvotes=week12_tbl$upvotes[doc_id]) %>%
  mutate(topic=as.factor(final_tbl$topic))

#statistical learning analysis
summary(lm(upvotes~topic,data=final_tbl)) #there was no significant difference found, meaning that upvotes did not significantly differ by topic

# machine learning analysis with elastic net
#creating test/train sets
split <- createDataPartition(final_tbl$upvotes,
                                       p = .25,
                                       list = T)$Resample1
test_tbl <- final_tbl[split,]
train_tbl <- final_tbl[-split,]

folds<-createFolds(train_tbl$upvotes) #creating folds 

elastic_net<- train(
  upvotes ~ topic,
  train_tbl,
  method="glmnet",
  na.action = na.pass,
  preProcess = c("center","scale","zv","nzv","medianImpute"),
  trControl = trainControl(method="cv", 
                           number=10, 
                           verboseIter=T, 
                           indexOut = folds)
)
cv_m2 <- max(elastic_net$results$Rsquared)
holdout_m2 <- cor(
  predict(elastic_net, test_tbl, na.action = na.pass),
  test_tbl$upvotes)^2
cv_m2
holdout_m2
#the CV was around 0.048 and the holdout was around 0.0024, which means that this model did a very bad job predicting topic from upvotes for both the training and testing data. This aligns with what we found in the statistical analysis, which is that upvotes cannot predict the topic.
```

# Visualization
```{r}
#creating a wordcloud
DTM_tbl <- io_dtm %>% as.matrix %>% as_tibble
wordcloud(
  words = names(DTM_tbl),
  freq = colSums(DTM_tbl),
  colors = brewer.pal(9,"Dark2")
)
#it seems like the most prominent words have to do with work, like jobs or career, which makes a lot of sense. People might be trying to find jobs in the field, get more information on what kind of jobs one can get within IO, or be talking about major topics within IO (job analysis, job domain, etc). Other frequent words are related to reddit forums in general, I would guess, as they relate to discussion. 
```

